{"name":"wikimedia-node-rdkafka","version":"0.3.1","description":"Node.js bindings for librdkafka","main":"lib/index.js","scripts":{"configure":"node-gyp configure","build":"node-gyp build","test":"make test","install":"node-gyp rebuild"},"keywords":["kafka","librdkafka"],"repository":"https://github.com/Blizzard/node-rdkafka","license":"MIT","devDependencies":{"jshint":"2.x","jsdoc":"^3.4.0","toolkit-jsdoc":"^1.0.0","mocha":"2.x","node-gyp":"3.x"},"dependencies":{"bindings":"1.x","nan":"2.x"},"engines":{"npm":"^2.7.3"},"gypfile":true,"gitHead":"1a7475ea4160b9ff2d7a3ed7a13e48d0553a348c","homepage":"https://github.com/Blizzard/node-rdkafka#readme","versions":[{"number":"0.3.1","date":"2016-08-23T16:30:48.801Z"}],"readme":"node-rdkafka - Node.js wrapper for Kafka C/C++ library\n==============================================\n\nCopyright (c) 2016 Blizzard Entertainment.\n\n[https://github.com/blizzard/node-rdkafka](https://github.com/blizzard/node-rdkafka)\n\n[![Build Status](https://travis-ci.org/Blizzard/node-rdkafka.svg?branch=master)](https://travis-ci.org/Blizzard/node-rdkafka)\n[![npm version](https://badge.fury.io/js/node-rdkafka.svg)](https://badge.fury.io/js/node-rdkafka)\n\n# Overview\n\nThe `node-rdkafka` library is a high-performance NodeJS client for [Apache Kafka](http://kafka.apache.org/) that wraps the native  [librdkafka](https://github.com/edenhill/librdkafka) library.  All the complexity of balancing writes across partitions and managing (possibly ever-changing) brokers should be encapsulated in the library.\n\n## Reference Docs\n\nTo view the reference docs for the current version, go [here](https://blizzard.github.io/node-rdkafka/current/)\n\n## Contributing\n\nFor guidelines on contributing please see [CONTRIBUTING.md](https://github.com/blizzard/node-rdkafka/blob/master/CONTRIBUTING.md)\n\n## Code of Conduct\n\nPlay nice; Play fair.\n\n## Requirements\n\n* Apache Kafka >=0.9\n* Node.js >=4\n* Linux/Mac (Sorry Windows :()\n\n## Tests\n\nThis project includes two types of unit tests in this project:\n* end-to-end integration tests\n* unit tests\n\nYou can run both types of tests by using `Makefile`. Doing so calls `mocha` in your locally installed `node_modules` directory.\n\n* Before you run the tests, be sure to init and update the submodules: \n  1. `git submodule init`\n  2. `git submodule update`\n* To run the unit tests, you can run `make lint` or `make test`.\n* To run the integration tests, you must have a running Kafka installation available. By default, the test tries to connect to `localhost:9092`; however, you can supply the `KAFKA_HOST` environment variable to override this default behavior.\n\n# Usage\n\nYou can install the `node-rdkafka` module like any other module:\n\n```\nnpm install node-rdkafka\n```\n\nTo use the module, you must `require` it.\n\n```js\nvar Kafka = require('node-rdkafka');\n```\n\n## Configuration\n\nYou can pass many configuration options to `librdkafka`.  A full list can be found at https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md.\n\nConfiguration keys that have the suffix `_cb` are designated as callbacks. Some\nof these keys are informational and you can choose to opt-in (for example, `dr_cb`). Others are callbacks designed to\nreturn a value, such as `partitioner_cb`.\n\nNot all of these options are supported.\nThe library will throw an error if the value you send in is invalid.\n\nThe library currently supports the following callbacks:\n* `partitioner_cb`\n* `dr_cb`\n* `event_cb`\n\n## Sending Messages\n\nA `Producer` sends messages to Kafka.  The `Producer` constructor takes a configuration object, as shown in the following example:\n\n```js\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': 'kafka-host1:9092,kafka-host2:9092'\n});\n```\n\nA `Producer` requires only `metadata.broker.list` (the Kafka brokers) to be created.  The values in this list are separated by commas.  For other configuration options, see the [Configuration.md](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md) file described previously.  \n\nThe following example illustrates a list with several `librdkafka` options set.\n\n```js\nvar producer = new Kafka.Producer({\n  'client.id': 'kafka',\n  'metadata.broker.list': 'localhost:9092',\n  'compression.codec': 'gzip',\n  'retry.backoff.ms': 200,\n  'message.send.max.retries': 10,\n  'socket.keepalive.enable': true,\n  'queue.buffering.max.messages': 100000,\n  'queue.buffering.max.ms': 1000,\n  'batch.num.messages': 1000000,\n  'dr_cb': true\n});\n```\n\n#### Stream API\n\nYou can easily use the `Producer` as a writable stream immediately after creation (as shown in the following example):\n\n```js\n// Our producer with its Kafka brokers\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': 'kafka-host1:9092,kafka-host2:9092'\n});\n\n// This call returns a new writable stream to our topic 'topic-name'\nvar stream = producer.getWriteStream('topic-name');\n\n// Writes a message to the stream\nvar queuedSuccess = stream.write(new Buffer('Awesome message'));\n\nif (queuedSuccess) {\n  console.log('We queued our message!');\n} else {\n  // Note that this only tells us if the stream's queue is full,\n  // it does NOT tell us if the message got to Kafka!  See below...\n  console.log('Too many messages in our queue already');\n}\n\nstream.on('error', function (err) {\n  // Here's where we'll know if something went wrong sending to Kafka\n  console.error('Error in our kafka stream');\n  console.error(err);\n})\n```\n\nNote that `getWriteStream` will create a new stream on every call.  You should try to cache the returned stream for a topic after the first call.\n\n#### Standard API\n\nThe Standard API is more performant, particularly when handling high volumes of messages.\nHowever, it requires more manual setup to use. The following example illustrates its use:\n\n```js\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': 'localhost:9092',\n  'dr_cb': true\n});\n\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  // Create a Topic object with any options our Producer\n  // should use when writing to that topic.\n  var topic = producer.Topic('topic', {\n    // Make the Kafka broker acknowledge our message (optional)\n    'request.required.acks': 1\n  });\n\n  producer.produce({\n    // Message to send. If a string is supplied, it will be\n    // converted to a Buffer automatically, but we're being\n    // explicit here for the sake of example.\n    message: new Buffer('Awesome message'),\n\n    // The topic object we created above\n    topic: topic\n  }, function(err) {\n    // Called after the message is queued\n    if (err) {\n      console.error('A problem occurred when sending our message');\n      console.error(err);\n    } else {\n      console.log('Message produced successfully!');\n    }\n  });\n});\n\n// Any errors we encounter, including connection errors\nproducer.on('error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n})\n```\n\nTo see the configuration options available to you, see the [Configuration](#configuration) section.\n\n##### Methods\n\n|Method|Description|\n|-------|----------|\n|`producer.connect()`| Connects to the broker. <br><br> The `connect()` method emits the `ready` event when it connects successfully or an `error` when it does not.|\n|`producer.disconnect()`| Disconnects from the broker. <br><br>The `disconnect()` method emits the `disconnected` event when it has disconnected or `error` if something went wrong. |\n|`producer.poll()` | Polls the producer for delivery reports or other events to be transmitted via the emitter. <br><br>This happens automatically on transactions such as `produce`. |\n|`producer.produce(msg)`| Sends a message. <br><br>The `produce()` method takes a JSON object in the format showed above. |\n\n##### Events\n\nSome configuration properties that end in `_cb` indicate that an event should be generated for that option.  You can either:\n\n* provide a value of `true` and react to the event\n* provide a callback function directly\n\nThe following example illustrates an event:\n\n```js\nvar producer = new Kafka.Producer({\n  'client.id': 'my-client', // Specifies an identifier to use to help trace activity in Kafka\n  'metadata.broker.list': 'localhost:9092', // Connect to a Kafka instance on localhost\n  'dr_cb': true // Specifies that we want a delivery-report event to be generated\n});\n\nproducer.on('delivery-report', function(report) {\n  // Report of delivery statistics here:\n  //\n  console.log(report);\n});\n```\n\nThe following table describes types of events.\n\n|Event|Description|\n|-------|----------|\n| `error` | Error reporting is handled through this pipeline. <br><br>Most errors will have a value for `code`, `message`, and `origin`. `origin` will be `local` or `kafka` to determine where the error happened. |\n| `disconnected` | The `disconnected` event is emitted when the broker has disconnected. <br><br>This event is emitted only when `.disconnect` is called. The wrapper will always try to reconnect otherwise. |\n| `ready` | The `ready` event is emitted when the `Producer` is ready to send messages. |\n| `event` | The `event` event is emitted when `librdkafka` reports an event (if you opted in via the `event_cb` option). |\n| `event.log` | The `event.log` event is emitted when logging events come in (if you opted into logging via the `event_cb` option). <br><br>You will need to set a value for `debug` if you want to send information. |\n| `event.status` | The  `event.status` event is emitted when `librdkafka` reports stats (if you opted in). |\n| `event.throttle` | The `event.throttle` event emitted  when `librdkafka` reports throttling. |\n| `delivery-report` | The `delivery-report` event is emitted when a delivery report has been found via polling. <br><br>To use this event, you must set `request.required.acks` to `1` or `-1` in topic configuration and `dr_cb` to `true` in the `Producer` constructor options. |\n\n## Kafka.KafkaConsumer\n\nTo read messages from Kafka, you use a `KafkaConsumer`.  You instantiate a `KafkaConsumer` object as follows:\n\n```js\nvar consumer = new Kafka.KafkaConsumer({\n  'group.id': 'kafka',\n  'metadata.broker.list': 'localhost:9092',\n}, {});\n```\n\nThe first parameter is the global config, while the second parameter is the topic config that gets applied to all subscribed topics. To view a list of all supported configuration properties, see the [Configuration.md](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md) file described previously. Look for the `C` and `*` keys.\n\nThe `group.id` and `metadata.broker.list` properties are required for a consumer.\n\n### Message Structure\n\nMessages that are returned by the `KafkaConsumer` have the following structure.\n\n```js\n{\n  message: new Buffer('hi'), // message contents as a Buffer\n  size: 2, // size of the message, in bytes\n  topic: 'librdtesting-01', // topic the message comes from\n  offset: 1337, // offset the message was read from\n  partition: 1 // partition the message was on\n}\n```\n\n### Stream API\n\nThe stream API is the easiest way to consume messages. The following example illustrates the use of the stream API:\n\n```js\n// Read from the librdtesting-01 topic... note that this creates a new stream on each call!\nvar stream = consumer.getReadStream('librdtesting-01');\n\nstream.on('data', function(data) {\n  console.log('Got message');\n  console.log(data.message.toString());\n});\n```\n\n### Standard API\n\nYou can also use the Standard API and manage callbacks and events yourself.  You can choose different modes for consuming messages:\n\n* *Flowing mode*. This mode flows all of the messages it can read by maintaining an infinite loop in the event loop. It only stops when it detects the consumer has issued the `unsubscribe` or `disconnect` method.\n* *Non-flowing mode*. This mode reads a single message from Kafka at a time manually.\n\nThe following example illustrates flowing mode:\n```js\n// Flowing mode\nconsumer.connect();\n\nconsumer\n  .on('ready', function() {\n    // Consume from the librdtesting-01 topic. This is what determines\n    // the mode we are running in. By consuming an entire topic,\n    // we will get messages from that topic as soon as they are available\n    consumer.consume('librdtesting-01');\n  })\n  .on('data', function(data) {\n    // Output the actual message contents\n    console.log(data.message.toString());\n  });\n```\nThe following example illustrates non-flowing mode:\n```js\n// Non-flowing mode\nconsumer.connect();\n\nconsumer\n  .on('ready', function() {\n    // Subscribe to the librdtesting-01 topic\n    // This makes subsequent consumes read from that topic.\n    consumer.subscribe('librdtesting-01');\n\n    // Read one message every 1000 seconds\n    setInterval(function() {\n      consumer.consume();\n    }, 1000);\n  })\n  .on('data', function(data) {\n    console.log('Message found!  Contents below.');\n    console.log(data.message.toString());\n  });\n```\n\nThe following table lists important methods for this API.\n\n|Method|Description|\n|-------|----------|\n|`consumer.connect()` | Connects to the broker. <br><br>The `connect()` emits the event `ready` when it has successfully connected, or an `error` when it has not. |\n|`consumer.disconnect()` | Disconnects from the broker. <br><br>The `disconnect()` method emits `disconnected` when it has disconnected or `error` if something went wrong.\n|`consumer.subscribe(topics, callback)` | Subscribes to an array of topics. <br><br> `topics` can be either an array or a string for a single topic. |\n|`consumer.unsubscribe()` | Unsubscribes from the currently subscribed topics. <br><br>You cannot subscribe to different topics without calling the `unsubscribe()` method first. |\n|`consumer.consume(cb)` | Gets a message from the existing subscription. |\n|`consumer.consume(topics, cb)` | Creates a subscription and get messages as they become available.<br><br>The `consume()` method keeps a background thread running to do the work. |\n\nThe following table lists events for this API.\n\n|Event|Description|\n|-------|----------|\n|`error` | Error reporting is handled through this pipeline. <br><br>Most errors will have a `code`, `message`, and `origin` value. The `origin` value will be **local** or **remote** to determine where the error happened. |\n|`disconnected` | The `disconnected` event is emitted when the broker disconnects. <br><br>This event is only emitted when `.disconnect` is called. The wrapper will always try to reconnect otherwise. |\n|`ready` | The `ready` event is emitted when the `Producer` is ready to send messages. |\n|`event` | The `event` event is emitted when `librdkafka` reports an event (if you opted in via the `event_cb` option).|\n|`event.log` | The `event.log` event is emitted when logging events occur (if you opted in for logging  via the `event_cb` option).<br><br> You will need to set a value for `debug` if you want information to send. |\n|`event.status` | The `event.status` event is emitted when `librdkafka` reports stats (if you opted in). |\n|`event.throttle` | The `event.throttle` event is emitted when `librdkafka` reports throttling.|\n\n## Metadata\n\nBoth `Kafka.Producer` and `Kafka.KafkaConsumer` include a `getMetadata` method to retrieve metadata from Kafka.\n\nGetting metadata on any connection returns the following data structure:\n\n```js\n{\n  orig_broker_id: 1,\n  orig_broker_name: \"broker_name\",\n  brokers: [\n    {\n      id: 1,\n      host: 'localhost',\n      port: 40\n    }\n  ],\n  topics: [\n    {\n      name: 'awesome-topic',\n      partitions: [\n        {\n          id: 1,\n          leader: 20,\n          replicas: [1, 2],\n          isrs: [1, 2]\n        }\n      ]\n    }\n  ]\n}\n```\n\nThe following example illustrates how to use the `getMetadata` method.\n\n```js\nvar opts = {\n  topic: 'librdtesting-01',\n  timeout: 10000\n};\n\nproducer.getMetadata(opts, function(err, metadata) {\n  if (err) {\n    console.error('Error getting metadata');\n    console.error(err);\n  } else {\n    console.log('Got metadata');\n    console.log(metadata);\n  }\n});\n```\n","created":"2016-08-23T16:30:48.801Z","modified":"2016-08-23T16:30:48.801Z","lastPublisher":{"name":"pchelolo","email":"petrpchelko@gmail.com"},"owners":[{"name":"pchelolo","email":"petrpchelko@gmail.com"}],"other":{"_attachments":{},"_from":".","_id":"wikimedia-node-rdkafka","_nodeVersion":"4.4.7","_npmOperationalInternal":{"host":"packages-12-west.internal.npmjs.com","tmp":"tmp/wikimedia-node-rdkafka-0.3.1.tgz_1471969848510_0.17927956604398787"},"_npmUser":{"name":"pchelolo","email":"petrpchelko@gmail.com"},"_npmVersion":"2.15.8","_rev":"1-9c806d7665fde2c5bb848a31a1b8a83c","_shasum":"bdb76bae1100a84869e8a3d1d861183b59489fa7","bugs":{"url":"https://github.com/Blizzard/node-rdkafka/issues"},"contributors":[{"name":"Stephen Parente","email":"webmakersteve@gmail.com"},{"name":"Matt Gollob","email":"mattness@users.noreply.github.com"}],"directories":{},"dist-tags":{"latest":"0.3.1"},"dist":{"shasum":"bdb76bae1100a84869e8a3d1d861183b59489fa7","tarball":"http://registry.npmjs.org/wikimedia-node-rdkafka/-/wikimedia-node-rdkafka-0.3.1.tgz"},"maintainers":[{"name":"pchelolo","email":"petrpchelko@gmail.com"}],"readmeFilename":"README.md","time":{"modified":"2016-08-23T16:30:48.801Z","created":"2016-08-23T16:30:48.801Z","0.3.1":"2016-08-23T16:30:48.801Z"}}}