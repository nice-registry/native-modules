{"name":"@ronomon/deduplication","version":"1.0.0","description":"Fast multi-threaded content-dependent chunking deduplication for Buffers in C++ with a reference implementation in Javascript. Ships with extensive tests, a fuzz test and a benchmark.","main":"binding.node","repository":"https://github.com/ronomon/deduplication","keywords":["async","backup","bandwidth","block size","boundary shift","cas","cdc","content-addressable storage","content-defined chunking","content-dependent chunking","cryptographic hash calculation","cut-point","deduplication","differential compression","elimination","fingerprint","gear hash","identical","maximum","minimum","normalized chunking","pattern","rabin karp","ratio","reference","reduce","redundancy","redundant","rsync","sliding block","sliding window","storage","tabulation hash","variable-sized chunking"],"license":"MIT","homepage":"https://github.com/ronomon/deduplication#readme","scripts":{"postinstall":"node-gyp clean","test":"node test.js","install":"node-gyp rebuild"},"dependencies":{"@ronomon/queue":"^2.1.0","nan":"^2.6.2"},"gypfile":true,"gitHead":"47e077cdb39437a4672935b2564881490f7bd95c","versions":[{"number":"1.0.0","date":"2017-08-17T13:05:19.238Z"}],"readme":"# deduplication\n\nFast multi-threaded content-dependent chunking deduplication for Buffers in C++ with a reference implementation in Javascript. Ships with extensive tests, a fuzz test and a benchmark.\n\n## Installation\n\n```\nnpm install @ronomon/deduplication\n```\n\n## Fast\n\n`@ronomon/deduplication` is an adaptation of [FastCDC](https://www.usenix.org/system/files/conference/atc16/atc16-paper-xia.pdf) written for Node.js as a native addon in C++.\n\n> FastCDC is about 10× faster than the best of open-source Rabin-based CDC, and about 3× faster\nthan the state-of-the-art Gear- and AE-based CDC, while achieving nearly the same deduplication ratio as the classic Rabin-based approach.\n\n`@ronomon/deduplication` achieves chunking speeds comparable to FastCDC, while the benchmark and performance results shown here also include the overhead of SHA-256 hashing:\n\n```\n           CPU: Intel(R) Xeon(R) CPU E3-1230 V2 @ 3.30GHz\n         Cores: 8\n       Threads: 8\n         Files: 64 x 4194304 Bytes\n\n============================================================\n\n         Chunk: 2048 Bytes (+1.42% E)\n         Ratio: 97.96%\n    Javascript: Latency: 196.375ms, Throughput: 160.36 MB/s\n        Native: Latency: 30.911ms, Throughput: 1028.49 MB/s\n\n         Chunk: 4096 Bytes (+7.57% E)\n         Ratio: 97.40%\n    Javascript: Latency: 164.572ms, Throughput: 192.70 MB/s\n        Native: Latency: 30.058ms, Throughput: 1073.74 MB/s\n\n         Chunk: 8192 Bytes (-1.76% E)\n         Ratio: 96.75%\n    Javascript: Latency: 150.184ms, Throughput: 211.03 MB/s\n        Native: Latency: 29.742ms, Throughput: 1086.78 MB/s\n\n         Chunk: 16384 Bytes (-4.88% E)\n         Ratio: 95.19%\n    Javascript: Latency: 142.566ms, Throughput: 222.58 MB/s\n        Native: Latency: 29.792ms, Throughput: 1091.20 MB/s\n\n         Chunk: 32768 Bytes (+20.57% E)\n         Ratio: 89.73%\n    Javascript: Latency: 139.043ms, Throughput: 227.68 MB/s\n        Native: Latency: 29.426ms, Throughput: 1104.67 MB/s\n\n         Chunk: 65536 Bytes (+3.61% E)\n         Ratio: 84.11%\n    Javascript: Latency: 138.047ms, Throughput: 229.63 MB/s\n        Native: Latency: 29.470ms, Throughput: 1100.15 MB/s\n\n         Chunk: 131072 Bytes (-16.45% E)\n         Ratio: 75.44%\n    Javascript: Latency: 135.161ms, Throughput: 233.42 MB/s\n        Native: Latency: 29.472ms, Throughput: 1100.15 MB/s\n```\n\n## Multi-threaded\n\nAll chunking and hashing algorithms are executed asynchronously in the Node.js threadpool for multi-core throughput, without blocking the event loop. This effectively treats the event loop as the *control plane* and the threadpool as the *data plane*. Multiple `source` buffers can be deduplicated across multiple threads by simply calling `deduplicate()` concurrently from the event loop. The number of `deduplicate()` calls in flight will be limited by the size of the threadpool, and further calls to `deduplicate()` will wait for these to finish before executing. Please see the [crypto-async](https://github.com/ronomon/crypto-async#adjust-threadpool-size-and-control-concurrency) module for advice on increasing the size of the Node.js threadpool.\n\n## Content-dependent chunking\n\nCompared to fixed size chunking, which fails to detect most of the same chunk cut-points when file content is shifted slightly, variable size content-dependent chunking can find most chunk cut-points no matter how the chunks move around. You can tune the absolute `minimum` and `maximum` chunk sizes required, as well as the expected `average` chunk size required.\n\nWhile content-dependent chunking is more CPU-intensive than fixed size chunking, the chunking algorithm of `@ronomon/deduplication` can detect chunks at a rate of more than 1.5 GB per second per CPU core. This is significantly faster than the SHA-256 hashing algorithm, which is 2.5× slower by way of contrast.\n\nThe following optimizations and variations on FastCDC are involved in the chunking algorithm:\n\n* 31 bit integers to avoid 64 bit integers for the sake of the Javascript reference implementation.\n\n* A right shift instead of a left shift to remove the need for an additional modulus operator, which would otherwise have been necessary to prevent overflow.\n\n* Masks are no longer zero-padded since a right shift is used instead of a left shift.\n\n* A more adaptive threshold based on a combination of `average` and `minimum` chunk size (rather than just `average` chunk size) to decide the pivot point at which to switch masks. A larger `minimum` chunk size now switches from the strict mask to the eager mask earlier.\n\n* Masks use 1 bit of chunk size normalization instead of 2 bits of chunk size normalization.\n\n## Deduplication\n\nThe 32 byte SHA-256 hash followed by the 4 byte `UInt32BE` size of each consecutive chunk will be written into the `target` buffer provided. You can use the SHA-256 hash combined with your own indexing scheme to determine whether a chunk should be stored on disk or transmitted across the network, and so reduce storage and bandwidth costs. You should apply deduplication before you apply compression.\n\n## Compression and average chunk size\n\nCompression and deduplication work in tension. Larger average chunk sizes achieve better compression ratios, while smaller average chunk sizes achieve better deduplication ratios. An `average` chunk size of 64 KB is recommended for optimal end-to-end deduplication and compression efficiency, according to the recommendations of [`Primary Data Deduplication - Large Scale Study and System Design`](https://www.usenix.org/system/files/conference/atc12/atc12-final293.pdf) by Microsoft. An `average` chunk size of 64 KB will not only maximize the combined savings from deduplication and compression, but will also minimize metadata overhead through reducing the average number of chunks considerably compared to typical `average` chunk sizes of 4 KB and 8 KB.\n\n## Invariants\n\nMost of these invariants are enforced with exceptions rather than asynchronous errors since they represent contract error rather than operational error:\n\n* `@ronomon/deduplication` will ensure that all chunks meet your `minimum` and `maximum` chunk size requirements (except for the last chunk in the last `source` buffer, which you can indicate by `flags=1`), and that the actual average chunk size is within ±20% of your expected `average` chunk size.\n\n* When tuning `average`, `minimum` and `maximum` chunk sizes, please ensure that the `(maximum - minimum > average)` invariant holds so that cut-points are not artificially forced instead of being content-dependent.\n\n* Please ensure that `average`, `minimum` and `maximum` chunk sizes are within the reasonable and inclusive bounds determined by the respective `_MIN` and `_MAX` constants defined in the Javascript reference and C++ implementations.\n\n* All integers, offsets and sizes must be at most 31 bits (2 GB) to avoid overflow and to optimize the Javascript reference implementation. Note that this does not place a limit on the size of file which can be deduplicated, since a file can be deduplicated in streaming fashion through multiple calls to `deduplicate()` (setting `flags=1` when the last `source` buffer is provided).\n\n* When deduplicating a file in streaming fashion through multiple calls to `deduplicate()`, please ensure that your `source` buffer is larger than the `maximum` chunk size required until such time as you set `flags=1` to indicate that the last `source` buffer has been provided (when it can be smaller).\n\n## Usage\n\nPlease try out the included demo (`node demo.js file`):\n\n```javascript\nvar assert = require('assert');\nvar fs = require('fs');\nvar path = require('path');\nvar Deduplication = require(path.join(module.filename, '..', 'binding.node'));\n\nvar file = process.argv[2];\nif (!file) {\n  console.error('usage: node demo.js file');\n  return;\n}\nvar fd = fs.openSync(file, 'r');\nvar fileOffset = 0;\nvar fileSize = fs.fstatSync(fd).size;\n\n// Recommended average, minimum and maximum chunk size constants:\nvar average = 65536;\nvar minimum = Math.round(average / 4);\nvar maximum = average * 8;\n\nvar source = Buffer.alloc(4 * 1024 * 1024);\nvar target = Buffer.alloc(Deduplication.targetSize(minimum, source.length));\n\nfunction close(error) {\n  fs.closeSync(fd);\n  if (error) throw error;\n}\n\nfunction read() {\n  var length = Math.min(source.length, fileSize - fileOffset);\n  assert(length >= 0);\n  if (length === 0) return close();\n  var sourceSize = fs.readSync(fd, source, 0, length, fileOffset);\n  if (sourceSize === 0) return close();\n  write(sourceSize);\n}\n\nfunction write(sourceSize) {\n  assert(fileOffset + sourceSize <= fileSize);\n  var flags = 0;\n  // We set flags = 1 to indicate if this is the last source buffer:\n  if (fileOffset + sourceSize === fileSize) flags |= 1;\n  Deduplication.deduplicate(\n    average,\n    minimum,\n    maximum,\n    source,\n    0,\n    sourceSize,\n    target,\n    0,\n    flags,\n    function(error, sourceOffset, targetOffset) {\n      if (error) return close(error);\n      assert(sourceOffset <= sourceSize);\n      assert(sourceOffset <= source.length);\n      assert(targetOffset <= target.length);\n      var offset = 0;\n      while (offset < targetOffset) {\n        var hash = target.toString('hex', offset, offset + 32);\n        offset += 32;\n        var size = target.readUInt32BE(offset);\n        offset += 4;\n        console.log('hash=' + hash + ' offset=' + fileOffset + ' size=' + size);\n        fileOffset += size;\n      }\n      assert(offset === targetOffset);\n      if (flags === 1) assert(fileOffset === fileSize);\n      read();\n    }\n  );\n}\n\nread();\n```\n\n## Tests\n\nTo test the native and Javascript bindings:\n\n```\nnode test.js\n```\n\n## Benchmark\n\nTo benchmark the native and Javascript bindings:\n\n```\nnode benchmark.js\n```\n","created":"2017-08-17T13:05:19.238Z","modified":"2017-08-17T13:05:19.238Z","lastPublisher":{"name":"ronomon","email":"info@ronomon.com"},"owners":[{"name":"ronomon","email":"info@ronomon.com"}],"other":{"_attachments":{},"_id":"@ronomon/deduplication","_nodeVersion":"8.2.1","_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/deduplication-1.0.0.tgz_1502975119163_0.19584565027616918"},"_npmUser":{"name":"ronomon","email":"info@ronomon.com"},"_npmVersion":"5.3.0","_rev":"1-f194e21af5d28df51ed91fdb0bbf0a3c","author":{"name":"Joran Dirk Greef"},"bugs":{"url":"https://github.com/ronomon/deduplication/issues"},"directories":{},"dist-tags":{"latest":"1.0.0"},"dist":{"integrity":"sha512-owHGYhV2c+qTmM6LNO7Tty7ur12JXxmY4WgRmsIVECqrDyVEt9Jgtd3ezEhFnqu8VQYfnPG6ZFV9Cqu9+DEAFA==","shasum":"02320d8b2a631f3c2b1bb3783b4a8c4a80b4c23a","tarball":"https://registry.npmjs.org/@ronomon/deduplication/-/deduplication-1.0.0.tgz"},"maintainers":[{"name":"ronomon","email":"info@ronomon.com"}],"readmeFilename":"README.md","time":{"modified":"2017-08-17T13:05:19.238Z","created":"2017-08-17T13:05:19.238Z","1.0.0":"2017-08-17T13:05:19.238Z"}}}