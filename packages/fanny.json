{"name":"fanny","version":"1.0.8","description":"FANN Fast Artificial Neural Network Node.JS Bindings","main":"./lib/index.js","keywords":["fann","ann","neural","network","ai","neural network"],"repository":"https://github.com/zipscene/fanny","scripts":{"test":"mocha","build":"node-gyp configure && node-gyp build","postinstall":"node-gyp configure && node-gyp build","preinstall":"./build_fann.sh"},"gypfile":true,"generatorInfo":{"name":"generator-zs-lib","version":"0.9.7"},"dependencies":{"common-schema":"^1.9.0","nan":"^2.5.0","pasync":"^1.4.1","xerror":"^1.1.2"},"devDependencies":{"chai":"^3.5.0","es6-promise":"^4.0.5","mocha":"^3.2.0","zstreams":"^3.2.2"},"license":"Apache-2.0","gitHead":"e8d2fc8003d8ad074c55f24114ea13d432f1c5bf","homepage":"https://github.com/zipscene/fanny#readme","versions":[{"number":"1.0.0","date":"2017-02-08T16:44:43.663Z"},{"number":"1.0.1","date":"2017-02-22T16:23:15.081Z"},{"number":"1.0.2","date":"2017-03-07T20:11:33.746Z"},{"number":"1.0.3","date":"2017-03-08T18:30:35.534Z"},{"number":"1.0.4","date":"2017-03-08T18:38:49.401Z"},{"number":"1.0.5","date":"2017-03-08T18:42:12.019Z"},{"number":"1.0.6","date":"2017-03-12T18:28:47.976Z"},{"number":"1.0.7","date":"2017-03-13T21:07:16.699Z"},{"number":"1.0.8","date":"2017-03-22T13:50:26.352Z"}],"readme":"# FANNy: Modern Node.JS Bindings for FANN (Fast Artificial Neural Network library)\n\n## Obligatory Brief Example\n\n```js\nvar fanny = require('fanny');\n// Create a neural network with 2 input nodes, 5 hidden nodes, and 1 output node\nvar ann = fanny.createANN({ layers: [ 2, 5, 1 ] });\n// Boolean XOR function training dataset\nvar dataset = [\n\t{ input: [ 0, 0 ], output: [ 0 ] },\n\t{ input: [ 0, 1 ], output: [ 1 ] },\n\t{ input: [ 1, 0 ], output: [ 1 ] },\n\t{ input: [ 1, 1 ], output: [ 0 ] }\n];\n// Train until a MSE (mean squared error) of 0.025.  Returns a Promise.\nann.train(fanny.createTrainingData(dataset), { desiredError: 0.025 })\n\t.then(function() {\n\t\t// Training complete.  Do some test runs.\n\t\t// (exact output is different each time due to random weight initialization)\n\t\tconsole.log(ann.run([ 1, 0 ])); // [ 0.906... ]\n\t\tconsole.log(ann.run([ 1, 1 ])); // [ 0.132... ]\n\t});\n```\n\n## Supported Features\n\nNearly all features of FANN are supported, including different datatypes (float, double, fixed) and\ntraining progress callbacks.  Multi-epoch training operations are asynchronous and run in a separate\nthread.  `run()` can be called either synchronously or asynchronously, as its speed can vary widely\ndepending on the network size.\n\n## Interfaces\n\nFANNy's primary interface roughly mirrors FANN's C++ interface, but with a number of changes and\ntweaks to make it better fit Javascript paradigms.  This is the interface described in the rest\nof this file.\n\nFANNy also provides a lower-level interface more directly in-line with FANN's own C++ interface.\nThis can be accessed by including the native addon itself: `require('fanny').getAddon('float')`.\n\n## Getting FANN\n\nThis module is currently built on FANN git as of Jan. 2017.  However, the original author no longer seems to be\nmaintaining the library, and several bugs in the current official version break some features of\nFANNy.  A maintained version with the appropriate bugfixes is available [here](https://github.com/crispy1989/fann).\n\nWhen you npm install fanny, it should automatically fetch and compile this version of FANN\nin a sandbox.  You will need standard build tools along with `cmake` installed.\n\n## Creating a Neural Network\n\nTo create a new neural network, use the `createANN()` function.  It takes two parameters.\nThe first (required) parameter is an object containing configuration information for the\nneural network.  The second parameter is an object containing a set of options.  See the\nsection on setting options for a list.\n\n```js\nvar config = {\n\ttype: 'standard', // 'standard' (default), 'sparse', or 'shortcut'\n\tlayers: [ 2, 10, 3, 1 ], // Sizes of layers in the neural network.  Required.\n\t// connectionRate: 0.5, // Connection rate, for sparse networks\n\tdatatype: 'float', // 'float' (default), 'double', or 'fixed', for libfloatfann, libdoublefann, libfixedfann, respectively\n\tactivationFunctions: { // override default activation functions for layers or individual neurons\n\t\t// Possible values: 'LINEAR', 'THRESHOLD', 'THRESHOLD_SYMMETRIC', 'SIGMOID', 'SIGMOID_STEPWISE',\n\t\t// 'SIGMOID_SYMMETRIC', 'SIGMOID_SYMMETRIC_STEPWISE', 'GAUSSIAN', 'GAUSSIAN_SYMMETRIC',\n\t\t// 'ELLIOT', 'ELLIOT_SYMMETRIC', 'LINEAR_PIECE', 'LINEAR_PIECE_SYMMETRIC', 'SIN_SYMMETRIC',\n\t\t// 'COS_SYMMETRIC', 'SIN', 'COS'\n\t\thidden: 'LINEAR', // all hidden nodes\n\t\toutput: 'THRESHOLD', // all output nodes\n\t\t'2': 'LINEAR', // nodes on layer 2 (indexed from 0)\n\t\t'1-2': 'LINEAR' // node 2 on layer 1\n\t},\n\tactivationSteepnesses: { // override default activation steepnesses for layers or individual neurons\n\t\t// keys are same as for activationFunctions\n\t\thidden: 0.2\n\t}\n};\nvar options = { ... };\nvar ann = fanny.createANN(config, options);\n```\n\n## Loading and Saving a Neural Network\n\nNeural networks are saved by default in floating points.  FANN fixed point saving can be enabled by\npassing boolean `true` as a second argument to `save()`.\n\n`fanny.loadANN()` will, by default, load neural networks using libfloatfann.  A second argument can\nbe passed with the datatype ('float', 'double', or 'fixed') to change this.\n\n```js\nann.save('/path/to/filename').then(...);\nfanny.loadANN('/path/to/filename').then(function(ann) { ... });\n```\n\n## Options\n\nMany of FANN's getter and setter functions are instead exposed as options that can easily\nbe set in batches.\n\n```js\nann.setOption(name, value);\nann.getOption(name);\nann.setOptions({ name1: value1, name2: value2, ... });\nann.getOptions(); // returns object\n```\n\nThe full list of possible options and available values can be found in `common-schema` format in\nthe file `src/ann.js`.\n\n## Training Data\n\nTraining data is represented by a `TrainingData` class.  It's constructed, saved, and loaded like\nan `ANN`:\n\n```js\nvar trainingData = fanny.createTrainingData(data);\ntrainingData.save('/path/to/filename').then(...);\nfanny.loadTrainingData('/path/to/filename').then(function(trainingData) { ... });\n```\n\nNote that training data must be instantiated with the same datatype as the ANN it's used with.\nThe functions `fanny.loadTrainingData()` and `fanny.createTrainingData()` both take an optional\nsecond argument containing the datatype, if different from the default ('float').\n\nThe `data` parameter can take several different formats of data:\n\n```js\nvar data1 = [\n\t{\n\t\tinput: [ 0.2, 0.7, ... ],\n\t\toutput: [ 0.3, 0.8, 0.5, ... ]\n\t},\n\t{\n\t\tinput: [ ... ],\n\t\toutput: [ ... ]\n\t},\n\t...\n];\nfanny.createTrainingData(data1);\n\nvar data2 = [\n\t[\n\t\t[ 0.2, 0.7, ... ],\n\t\t[ 0.3, 0.8, 0.5, ... ]\n\t],\n\t[\n\t\t[ ... ],\n\t\t[ ... ]\n\t],\n\t...\n];\nfanny.createTrainingData(data2);\n\nvar inputs = [\n\t[ 0.2, 0.7, ... ],\n\t...\n];\nvar outputs = [\n\t[ 0.3, 0.8, 0.5, ... ],\n\t...\n];\nfanny.createTrainingData(inputs, outputs);\n```\n\n`TrainingData` also has several other methods that can get and manipulate the data.  These\nare direct equivalents of their corresponding FANN functions.  Here are the available functions:\n\n- `shuffle()`\n- `merge()`\n- `getLength()`\n- `getNumInputs()`\n- `getNumOutputs()`\n- `getInputData()`\n- `getOutputData()`\n- `getOneInputData()`\n- `getOneOutputData()`\n- `getMinInput()`\n- `getMaxInput()`\n- `getMinOutput()`\n- `getMaxOutput()`\n- `scaleInput()`\n- `scaleOutput()`\n- `scale()`\n- `subset()`\n- `setData()`\n- `clone()`\n\n## Training\n\nTraining a single datapair is easy and synchronous:\n\n```js\nann.trainOne([ <inputs>, <outputs> ]);\n```\n\nTraining a single epoch with a training dataset returns a Promise:\n\n```js\nann.train(trainingData).then(...);\n```\n\nTo train for multiple epochs, you can pass a set of options to `train()` as a\nsecond argument.  All options are optional, but it is highly recommended to set at least\n`desiredError`.\n\n```js\nann.train(trainingData, {\n\tdesiredError: 0.05, // Stop training when this error (MSE by default) is reached\n\tmaxEpochs: 1000, // Stop training if we do this many epochs\n\tstopFunction: 'MSE', // Determines the meaning of desiredError.  MSE is default.  'BIT' is for bitfail.\n\tcascade: false, // enable cascade training\n\t//maxNeurons: 100000, // Used instead of maxEpochs when cascade training\n\tprogressInterval: 1 // Number of epochs between calling the progress function\n}).then(...);\n```\n\n`train()` can also be given a third argument, a callback function that is called periodically\nduring training (defined by `progressInterval`).\n\n```js\nvar progressFn = function(info) {\n\tconsole.log(info.iteration, info.mse, info.bitfail);\n};\n```\n\nThe progress function can optionally return `false` to cancel training (and immediately reject the promise).\n\nInstead of passing a progress function as the third argument, the special value 'default' can be\npassed (as a string) to enable FANN's default behavior of printing status reports to stdout.\n\n## Running\n\nThe neural network can be run either synchronously or asynchronously:\n\n```js\nvar outputs = ann.run(inputs);\nann.runAsync(inputs).then(function(outputs) { ... });\n```\n\n## Getting Current Information and Stats\n\nThe `ANN` object has a property called `info` containing current information about the network.  Keys include:\n\n- `numInput`\n- `numOutput`\n- `totalNeurons`\n- `totalConnections`\n- `decimalPoint`\n- `multiplier`\n- `networkType`\n- `connectionRate`\n- `numLayers`\n\nEach of these corresponds to a FANN getter.\n\n## User Data\n\nThe `ANN` object has a property called `userData` which is initialized to an empty object.  You can store\nany data you need to in there, and it will be saved and loaded with the neural network.  The data is\nstored as a JSON object in the FANN `user_data_string` field.\n\n## Other ANN Functions\n\nThese functions correspond directly (insofar as translation to Javascript allows) to functions\non the FANN C++ `neural_net` class.\n\n- `randomizeWeights`\n- `initWeights`\n- `printConnections`\n- `getMSE`\n- `resetMSE`\n- `printParameters`\n- `getActivationFunction`\n- `setActivationFunction`\n- `setActivationFunctionLayer`\n- `setActivationFunctionHidden`\n- `setActivationFunctionOutput`\n- `getActivationSteepness`\n- `setActivationSteepness`\n- `setActivationSteepnessLayer`\n- `setActivationSteepnessHidden`\n- `setActivationSteepnessOutput`\n- `getLayerArray`\n- `getBiasArray`\n- `getConnectionArray`\n- `setWeightArray`\n- `setWeight`\n- `scaleTrainingData`\n- `descaleTrainingData`\n- `setInputScalingParams`\n- `setOutputScalingParams`\n- `setScalingParams`\n- `clearScalingParams`\n- `scaleInput`\n- `scaleOutput`\n- `descaleInput`\n- `descaleOutput`\n- `testOne`\n- `testData`\n\n\n","created":"2017-02-08T16:44:43.663Z","modified":"2017-03-22T13:50:26.352Z","lastPublisher":{"name":"crispy1989","email":"crispy@cluenet.org"},"owners":[{"name":"crispy1989","email":"crispy@cluenet.org"}],"other":{"_attachments":{},"_from":".","_id":"fanny","_nodeVersion":"4.7.3","_npmOperationalInternal":{"host":"packages-18-east.internal.npmjs.com","tmp":"tmp/fanny-1.0.8.tgz_1490190625627_0.018195857759565115"},"_npmUser":{"name":"crispy1989","email":"crispy@cluenet.org"},"_npmVersion":"2.15.11","_rev":"9-19ed4a291b1cf6a7930efeedcb2357d1","_shasum":"f146297894c709c51165f2f6dbaa451181a2c61b","bugs":{"url":"https://github.com/zipscene/fanny/issues"},"contributors":[{"name":"Chris Breneman","email":"crispy@cluenet.org"},{"name":"Erin Thompson","email":"thompsonerin42@gmail.com"},{"name":"Nick Jutte","email":"nick.jutte@gmail.com"}],"directories":{},"dist-tags":{"latest":"1.0.8"},"dist":{"shasum":"f146297894c709c51165f2f6dbaa451181a2c61b","tarball":"https://registry.npmjs.org/fanny/-/fanny-1.0.8.tgz"},"maintainers":[{"name":"crispy1989","email":"crispy@cluenet.org"}],"readmeFilename":"README.md","time":{"modified":"2017-03-22T13:50:26.352Z","created":"2017-02-08T16:44:43.663Z","1.0.0":"2017-02-08T16:44:43.663Z","1.0.1":"2017-02-22T16:23:15.081Z","1.0.2":"2017-03-07T20:11:33.746Z","1.0.3":"2017-03-08T18:30:35.534Z","1.0.4":"2017-03-08T18:38:49.401Z","1.0.5":"2017-03-08T18:42:12.019Z","1.0.6":"2017-03-12T18:28:47.976Z","1.0.7":"2017-03-13T21:07:16.699Z","1.0.8":"2017-03-22T13:50:26.352Z"}}}