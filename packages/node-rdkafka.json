{"name":"node-rdkafka","version":"1.0.6","description":"Node.js bindings for librdkafka","main":"lib/index.js","scripts":{"configure":"node-gyp configure","build":"node-gyp build","test":"make test","install":"node-gyp rebuild"},"keywords":["kafka","librdkafka"],"repository":"https://github.com/Blizzard/node-rdkafka","license":"MIT","devDependencies":{"jshint":"2.x","jsdoc":"^3.4.0","toolkit-jsdoc":"^1.0.0","mocha":"2.x","node-gyp":"3.x"},"dependencies":{"bindings":"1.x","nan":"2.x"},"engines":{"npm":"^2.7.3"},"gypfile":true,"gitHead":"e011486a2f6aef192e9fa7b89cfbc283679d99c9","homepage":"https://github.com/Blizzard/node-rdkafka#readme","versions":[{"number":"0.3.0","date":"2016-08-11T18:05:07.341Z"},{"number":"0.3.1","date":"2016-08-17T19:03:51.613Z"},{"number":"0.3.2","date":"2016-08-25T23:24:02.791Z"},{"number":"0.3.3","date":"2016-08-26T01:15:24.640Z"},{"number":"0.4.0","date":"2016-09-22T14:37:31.700Z"},{"number":"0.4.1","date":"2016-09-22T23:59:20.191Z"},{"number":"0.4.2","date":"2016-09-23T00:58:34.881Z"},{"number":"0.5.0-ALPHA","date":"2016-09-28T17:25:13.035Z"},{"number":"0.5.0-BETA","date":"2016-09-28T17:26:39.166Z"},{"number":"0.5.0","date":"2016-09-28T17:50:46.295Z"},{"number":"0.5.1","date":"2016-09-29T00:42:32.782Z"},{"number":"0.5.2","date":"2016-10-04T22:05:10.218Z"},{"number":"0.5.3","date":"2016-10-04T23:13:41.051Z"},{"number":"0.5.4","date":"2016-10-05T21:21:15.743Z"},{"number":"0.5.5","date":"2016-10-11T00:40:57.740Z"},{"number":"0.6.0","date":"2016-11-15T01:14:49.039Z"},{"number":"0.6.1","date":"2016-11-22T19:04:28.015Z"},{"number":"0.6.2","date":"2016-12-16T01:46:42.565Z"},{"number":"0.6.3","date":"2016-12-23T00:17:54.299Z"},{"number":"0.6.4","date":"2017-01-18T21:05:09.402Z"},{"number":"0.7.0-ALPHA.2","date":"2017-01-19T21:38:00.113Z"},{"number":"0.7.0-ALPHA.3","date":"2017-02-08T21:06:57.466Z"},{"number":"0.8.0","date":"2017-02-27T23:24:02.659Z"},{"number":"0.8.1","date":"2017-03-21T21:08:28.200Z"},{"number":"0.8.2","date":"2017-03-21T21:40:35.905Z"},{"number":"0.8.2-TWFIX","date":"2017-03-21T22:41:03.841Z"},{"number":"0.9.0","date":"2017-03-22T21:38:05.071Z"},{"number":"0.10.0","date":"2017-03-25T06:29:40.713Z"},{"number":"0.10.1","date":"2017-04-27T07:32:33.880Z"},{"number":"0.10.2","date":"2017-04-27T15:46:32.996Z"},{"number":"1.0.0","date":"2017-06-01T20:12:30.406Z"},{"number":"1.0.1","date":"2017-06-20T16:37:39.834Z"},{"number":"1.0.2","date":"2017-06-22T16:38:08.997Z"},{"number":"1.0.3","date":"2017-06-23T03:44:42.237Z"},{"number":"1.0.4","date":"2017-06-23T19:36:47.914Z"},{"number":"1.0.6","date":"2017-07-18T03:59:12.240Z"}],"readme":"node-rdkafka - Node.js wrapper for Kafka C/C++ library\n==============================================\n\nCopyright (c) 2016 Blizzard Entertainment.\n\n[https://github.com/blizzard/node-rdkafka](https://github.com/blizzard/node-rdkafka)\n\n[![Build Status](https://travis-ci.org/Blizzard/node-rdkafka.svg?branch=master)](https://travis-ci.org/Blizzard/node-rdkafka)\n[![npm version](https://badge.fury.io/js/node-rdkafka.svg)](https://badge.fury.io/js/node-rdkafka)\n\n# Overview\n\nThe `node-rdkafka` library is a high-performance NodeJS client for [Apache Kafka](http://kafka.apache.org/) that wraps the native  [librdkafka](https://github.com/edenhill/librdkafka) library.  All the complexity of balancing writes across partitions and managing (possibly ever-changing) brokers should be encapsulated in the library.\n\n__This library currently uses `librdkafka` version `0.9.5`.__\n\n## Reference Docs\n\nTo view the reference docs for the current version, go [here](https://blizzard.github.io/node-rdkafka/current/)\n\n## Contributing\n\nFor guidelines on contributing please see [CONTRIBUTING.md](https://github.com/blizzard/node-rdkafka/blob/master/CONTRIBUTING.md)\n\n## Code of Conduct\n\nPlay nice; Play fair.\n\n## Requirements\n\n* Apache Kafka >=0.9\n* Node.js >=4\n* Linux/Mac (Sorry Windows :()\n\n## Tests\n\nThis project includes two types of unit tests in this project:\n* end-to-end integration tests\n* unit tests\n\nYou can run both types of tests by using `Makefile`. Doing so calls `mocha` in your locally installed `node_modules` directory.\n\n* Before you run the tests, be sure to init and update the submodules:\n  1. `git submodule init`\n  2. `git submodule update`\n* To run the unit tests, you can run `make lint` or `make test`.\n* To run the integration tests, you must have a running Kafka installation available. By default, the test tries to connect to `localhost:9092`; however, you can supply the `KAFKA_HOST` environment variable to override this default behavior.\n\n# Usage\n\nYou can install the `node-rdkafka` module like any other module:\n\n```\nnpm install node-rdkafka\n```\n\nTo use the module, you must `require` it.\n\n```js\nvar Kafka = require('node-rdkafka');\n```\n\n## Configuration\n\nYou can pass many configuration options to `librdkafka`.  A full list can be found in `librdkafka`'s [Configuration.md](https://github.com/edenhill/librdkafka/blob/0.9.5.x/CONFIGURATION.md)\n\nConfiguration keys that have the suffix `_cb` are designated as callbacks. Some\nof these keys are informational and you can choose to opt-in (for example, `dr_cb`). Others are callbacks designed to\nreturn a value, such as `partitioner_cb`.\n\nNot all of these options are supported.\nThe library will throw an error if the value you send in is invalid.\n\nThe library currently supports the following callbacks:\n* `partitioner_cb`\n* `dr_cb` or `dr_msg_cb`\n* `event_cb`\n\n### SASL Support\n\n`librdkafka` supports using SASL for authentication and `node-rdkafka` has it turned on by default. If you would like\ndisable `sasl` support, export `WITH_SASL=0` before you run `npm install`. (You can also specify it when using `node-gyp`, `node-gyp --WITH_SASL=0 rebuild`)\n\nThis means you are required to have `libsasl2` on the machine before you build it.\n\n### Librdkafka Methods\n\nThis library includes two utility functions for detecting the status of your installation. Please try to include these when making issue reports where applicable.\n\nYou can get the features supported by your compile of `librdkafka` by reading the variable \"features\" on the root of the `node-rdkafka` object.\n\n```js\nconst kafka = require('node-rdkafka');\nconsole.log(kafka.features);\n\n// #=> [ 'gzip', 'snappy', 'ssl', 'sasl', 'regex', 'lz4' ]\n```\n\nYou can also get the version of `librdkafka`\n\n```js\nconst kafka = require('node-rdkafka');\nconsole.log(kafka.librdkafkaVersion);\n\n// #=> 0.9.5\n```\n\n## Sending Messages\n\nA `Producer` sends messages to Kafka.  The `Producer` constructor takes a configuration object, as shown in the following example:\n\n```js\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': 'kafka-host1:9092,kafka-host2:9092'\n});\n```\n\nA `Producer` requires only `metadata.broker.list` (the Kafka brokers) to be created.  The values in this list are separated by commas.  For other configuration options, see the [Configuration.md](https://github.com/edenhill/librdkafka/blob/0.9.4.x/CONFIGURATION.md) file described previously.\n\nThe following example illustrates a list with several `librdkafka` options set.\n\n```js\nvar producer = new Kafka.Producer({\n  'client.id': 'kafka',\n  'metadata.broker.list': 'localhost:9092',\n  'compression.codec': 'gzip',\n  'retry.backoff.ms': 200,\n  'message.send.max.retries': 10,\n  'socket.keepalive.enable': true,\n  'queue.buffering.max.messages': 100000,\n  'queue.buffering.max.ms': 1000,\n  'batch.num.messages': 1000000,\n  'dr_cb': true\n});\n```\n\n#### Stream API\n\nYou can easily use the `Producer` as a writable stream immediately after creation (as shown in the following example):\n\n```js\n// Our producer with its Kafka brokers\n// This call returns a new writable stream to our topic 'topic-name'\nvar stream = Kafka.Producer.createWriteStream({\n  'metadata.broker.list': 'kafka-host1:9092,kafka-host2:9092'\n}, {}, {\n  topic: 'topic-name'\n});\n\n// Writes a message to the stream\nvar queuedSuccess = stream.write(new Buffer('Awesome message'));\n\nif (queuedSuccess) {\n  console.log('We queued our message!');\n} else {\n  // Note that this only tells us if the stream's queue is full,\n  // it does NOT tell us if the message got to Kafka!  See below...\n  console.log('Too many messages in our queue already');\n}\n\nstream.on('error', function (err) {\n  // Here's where we'll know if something went wrong sending to Kafka\n  console.error('Error in our kafka stream');\n  console.error(err);\n})\n```\n\n#### Standard API\n\nThe Standard API is more performant, particularly when handling high volumes of messages.\nHowever, it requires more manual setup to use. The following example illustrates its use:\n\n```js\nvar producer = new Kafka.Producer({\n  'metadata.broker.list': 'localhost:9092',\n  'dr_cb': true\n});\n\n// Connect to the broker manually\nproducer.connect();\n\n// Wait for the ready event before proceeding\nproducer.on('ready', function() {\n  try {\n    producer.produce(\n      // Topic to send the message to\n      'topic',\n      // optionally we can manually specify a partition for the message\n      // this defaults to -1 - which will use librdkafka's default partitioner (consistent random for keyed messages, random for unkeyed messages)\n      null,\n      // Message to send. Must be a buffer\n      new Buffer('Awesome message'),\n      // for keyed messages, we also specify the key - note that this field is optional\n      'Stormwind',\n      // you can send a timestamp here. If your broker version supports it,\n      // it will get added. Otherwise, we default to 0\n      Date.now(),\n      // you can send an opaque token here, which gets passed along\n      // to your delivery reports\n    );\n  } catch (err) {\n    console.error('A problem occurred when sending our message');\n    console.error(err);\n  }\n});\n\n// Any errors we encounter, including connection errors\nproducer.on('event.error', function(err) {\n  console.error('Error from producer');\n  console.error(err);\n})\n```\n\nTo see the configuration options available to you, see the [Configuration](#configuration) section.\n\n##### Methods\n\n|Method|Description|\n|-------|----------|\n|`producer.connect()`| Connects to the broker. <br><br> The `connect()` method emits the `ready` event when it connects successfully or an `error` when it does not.|\n|`producer.disconnect()`| Disconnects from the broker. <br><br>The `disconnect()` method emits the `disconnected` event when it has disconnected or `error` if something went wrong. |\n|`producer.poll()` | Polls the producer for delivery reports or other events to be transmitted via the emitter. <br><br>In order to get the events in `librdkafka`'s queue to emit, you must call this regularly. |\n|`producer.setPollInterval(interval)` | Polls the producer on this interval, handling disconnections and reconnection. Set it to 0 to turn it off. |\n|`producer.produce(topic, partition, msg, key, timestamp, opaque)`| Sends a message. <br><br>The `produce()` method throws when produce would return an error. Ordinarily, this is just if the queue is full. |\n|`producer.flush(timeout, callback)`| Flush the librdkafka internal queue, sending all messages. Default timeout is 500ms |\n\n##### Events\n\nSome configuration properties that end in `_cb` indicate that an event should be generated for that option.  You can either:\n\n* provide a value of `true` and react to the event\n* provide a callback function directly\n\nThe following example illustrates an event:\n\n```js\nvar producer = new Kafka.Producer({\n  'client.id': 'my-client', // Specifies an identifier to use to help trace activity in Kafka\n  'metadata.broker.list': 'localhost:9092', // Connect to a Kafka instance on localhost\n  'dr_cb': true // Specifies that we want a delivery-report event to be generated\n});\n\n// Poll for events every 100 ms\nproducer.setPollInterval(100);\n\nproducer.on('delivery-report', function(err, report) {\n  // Report of delivery statistics here:\n  //\n  console.log(report);\n});\n```\n\nThe following table describes types of events.\n\n|Event|Description|\n|-------|----------|\n| `error` | Error reporting is handled through this pipeline. <br><br>Most errors will have a value for `code`, `message`, and `origin`. `origin` will be `local` or `kafka` to determine where the error happened. |\n| `disconnected` | The `disconnected` event is emitted when the broker has disconnected. <br><br>This event is emitted only when `.disconnect` is called. The wrapper will always try to reconnect otherwise. |\n| `ready` | The `ready` event is emitted when the `Producer` is ready to send messages. |\n| `event` | The `event` event is emitted when `librdkafka` reports an event (if you opted in via the `event_cb` option). |\n| `event.log` | The `event.log` event is emitted when logging events come in (if you opted into logging via the `event_cb` option). <br><br>You will need to set a value for `debug` if you want to send information. |\n| `event.stats` | The  `event.stats` event is emitted when `librdkafka` reports stats (if you opted in). |\n| `event.error` | The  `event.error` event is emitted when `librdkafka` reports an error |\n| `event.throttle` | The `event.throttle` event emitted  when `librdkafka` reports throttling. |\n| `delivery-report` | The `delivery-report` event is emitted when a delivery report has been found via polling. <br><br>To use this event, you must set `request.required.acks` to `1` or `-1` in topic configuration and `dr_cb` (or `dr_msg_db` if you want the report to contain the message payload) to `true` in the `Producer` constructor options. |\n\n## Kafka.KafkaConsumer\n\nTo read messages from Kafka, you use a `KafkaConsumer`.  You instantiate a `KafkaConsumer` object as follows:\n\n```js\nvar consumer = new Kafka.KafkaConsumer({\n  'group.id': 'kafka',\n  'metadata.broker.list': 'localhost:9092',\n}, {});\n```\n\nThe first parameter is the global config, while the second parameter is the topic config that gets applied to all subscribed topics. To view a list of all supported configuration properties, see the [Configuration.md](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md) file described previously. Look for the `C` and `*` keys.\n\nThe `group.id` and `metadata.broker.list` properties are required for a consumer.\n\n### Rebalancing\n\nRebalancing is managed internally by `librdkafka` by default. If you would like to override this functionality, you may provide your own logic as a rebalance callback.\n\n```js\nvar consumer = new Kafka.KafkaConsumer({\n  'group.id': 'kafka',\n  'metadata.broker.list': 'localhost:9092',\n  'rebalance_cb': function(err, assignment) {\n\n    if (err.code === Kafka.CODES.ERRORS.ERR__ASSIGN_PARTITIONS) {\n      // Note: this can throw when you are disconnected. Take care and wrap it in\n      // a try catch if that matters to you\n      this.assign(assignment);\n    } else if (err.code == Kafka.CODES.ERRORS.ERR__REVOKE_PARTITIONS){\n      // Same as above\n      this.unassign();\n    } else {\n      // We had a real error\n      console.error(err);\n    }\n\n  }\n})\n```\n\n`this` is bound to the `KafkaConsumer` you have created. By specifying a `rebalance_cb` you can also listen to the `rebalance` event as an emitted event. This event is not emitted when using the internal `librdkafka` rebalancer.\n\n### Commits\n\nWhen you commit in `node-rdkafka`, the standard way is to queue the commit request up with the next `librdkafka` request to the broker. When doing this, there isn't a way to know the result of the commit. Luckily there is another callback you can listen to to get this information\n\n```js\nvar consumer = new Kafka.KafkaConsumer({\n  'group.id': 'kafka',\n  'metadata.broker.list': 'localhost:9092',\n  'offset_commit_cb': function(err, topicPartitions) {\n\n    if (err) {\n      // There was an error committing\n      console.error(err);\n    } else {\n      // Commit went through. Let's log the topic partitions\n      console.log(topicPartitions);\n    }\n\n  }\n})\n```\n\n`this` is bound to the `KafkaConsumer` you have created. By specifying an `offset_commit_cb` you can also listen to the `offset.commit` event as an emitted event. It also has an error parameter and a list of topic partitions. This is not emitted unless opted in.\n\n### Message Structure\n\nMessages that are returned by the `KafkaConsumer` have the following structure.\n\n```js\n{\n  value: new Buffer('hi'), // message contents as a Buffer\n  size: 2, // size of the message, in bytes\n  topic: 'librdtesting-01', // topic the message comes from\n  offset: 1337, // offset the message was read from\n  partition: 1, // partition the message was on\n  key: 'someKey' // key of the message if present\n}\n```\n\n### Stream API\n\nThe stream API is the easiest way to consume messages. The following example illustrates the use of the stream API:\n\n```js\n// Read from the librdtesting-01 topic... note that this creates a new stream on each call!\nvar stream = KafkaConsumer.createReadStream(globalConfig, topicConfig, {\n  topics: ['librdtesting-01']\n});\n\nstream.on('data', function(message) {\n  console.log('Got message');\n  console.log(message.value.toString());\n});\n```\n\n### Standard API\n\nYou can also use the Standard API and manage callbacks and events yourself.  You can choose different modes for consuming messages:\n\n* *Flowing mode*. This mode flows all of the messages it can read by maintaining an infinite loop in the event loop. It only stops when it detects the consumer has issued the `unsubscribe` or `disconnect` method.\n* *Non-flowing mode*. This mode reads a single message from Kafka at a time manually.\n\nThe following example illustrates flowing mode:\n```js\n// Flowing mode\nconsumer.connect();\n\nconsumer\n  .on('ready', function() {\n    consumer.subscribe(['librdtesting-01']);\n\n    // Consume from the librdtesting-01 topic. This is what determines\n    // the mode we are running in. By not specifying a callback (or specifying\n    // only a callback) we get messages as soon as they are available.\n    consumer.consume();\n  })\n  .on('data', function(data) {\n    // Output the actual message contents\n    console.log(data.message.toString());\n  });\n```\nThe following example illustrates non-flowing mode:\n```js\n// Non-flowing mode\nconsumer.connect();\n\nconsumer\n  .on('ready', function() {\n    // Subscribe to the librdtesting-01 topic\n    // This makes subsequent consumes read from that topic.\n    consumer.subscribe(['librdtesting-01']);\n\n    // Read one message every 1000 milliseconds\n    setInterval(function() {\n      consumer.consume(1);\n    }, 1000);\n  })\n  .on('data', function(data) {\n    console.log('Message found!  Contents below.');\n    console.log(data.message.toString());\n  });\n```\n\nThe following table lists important methods for this API.\n\n|Method|Description|\n|-------|----------|\n|`consumer.connect()` | Connects to the broker. <br><br>The `connect()` emits the event `ready` when it has successfully connected, or an `error` when it has not. |\n|`consumer.disconnect()` | Disconnects from the broker. <br><br>The `disconnect()` method emits `disconnected` when it has disconnected or `error` if something went wrong.\n|`consumer.subscribe(topics)` | Subscribes to an array of topics. |\n|`consumer.unsubscribe()` | Unsubscribes from the currently subscribed topics. <br><br>You cannot subscribe to different topics without calling the `unsubscribe()` method first. |\n|`consumer.consume(cb)` | Gets messages from the existing subscription as quickly as possible. This method keeps a background thread running to do the work. If `cb` is specified, invokes `cb(err, message)`. |\n|`consumer.consume(number, cb)` | Gets `number` of messages from the existing subscription. If `cb` is specified, invokes `cb(err, message)`. |\n|`consumer.commit()` | Commits all locally stored offsets |\n|`consumer.commit(topicPartition)` | Commits offsets specified by the topic partition |\n|`consumer.commitMessage(message)` | Commits the offsets specified by the message |\n\nThe following table lists events for this API.\n\n|Event|Description|\n|-------|----------|\n|`error` | Error reporting is handled through this pipeline. <br><br>Most errors will have a `code`, `message`, and `origin` value. The `origin` value will be **local** or **remote** to determine where the error happened. |\n|`disconnected` | The `disconnected` event is emitted when the broker disconnects. <br><br>This event is only emitted when `.disconnect` is called. The wrapper will always try to reconnect otherwise. |\n|`ready` | The `ready` event is emitted when the `Producer` is ready to send messages. |\n|`event` | The `event` event is emitted when `librdkafka` reports an event (if you opted in via the `event_cb` option).|\n|`event.log` | The `event.log` event is emitted when logging events occur (if you opted in for logging  via the `event_cb` option).<br><br> You will need to set a value for `debug` if you want information to send. |\n|`event.stats` | The `event.stats` event is emitted when `librdkafka` reports stats (if you opted in). |\n|`event.throttle` | The `event.throttle` event is emitted when `librdkafka` reports throttling.|\n\n## Metadata\n\nBoth `Kafka.Producer` and `Kafka.KafkaConsumer` include a `getMetadata` method to retrieve metadata from Kafka.\n\nGetting metadata on any connection returns the following data structure:\n\n```js\n{\n  orig_broker_id: 1,\n  orig_broker_name: \"broker_name\",\n  brokers: [\n    {\n      id: 1,\n      host: 'localhost',\n      port: 40\n    }\n  ],\n  topics: [\n    {\n      name: 'awesome-topic',\n      partitions: [\n        {\n          id: 1,\n          leader: 20,\n          replicas: [1, 2],\n          isrs: [1, 2]\n        }\n      ]\n    }\n  ]\n}\n```\n\nThe following example illustrates how to use the `getMetadata` method.\n\nWhen fetching metadata for a specific topic, if a topic reference does not exist, one is created using the default config.\nPlease see the documentation on `Client.getMetadata` if you want to set configuration parameters, e.g. `acks`, on a topic to produce messages to.\n\n```js\nvar opts = {\n  topic: 'librdtesting-01',\n  timeout: 10000\n};\n\nproducer.getMetadata(opts, function(err, metadata) {\n  if (err) {\n    console.error('Error getting metadata');\n    console.error(err);\n  } else {\n    console.log('Got metadata');\n    console.log(metadata);\n  }\n});\n```\n","starsCount":3,"created":"2016-08-11T18:05:07.341Z","modified":"2017-08-05T15:39:02.483Z","lastPublisher":{"name":"webmakersteve","email":"webmakersteve@gmail.com"},"owners":[{"name":"webmakersteve","email":"stephen@91ferns.com"}],"other":{"_attachments":{},"_from":".","_id":"node-rdkafka","_nodeVersion":"6.2.0","_npmOperationalInternal":{"host":"s3://npm-registry-packages","tmp":"tmp/node-rdkafka-1.0.6.tgz_1500350351665_0.13636331167072058"},"_npmUser":{"name":"webmakersteve","email":"webmakersteve@gmail.com"},"_npmVersion":"3.8.9","_rev":"26-478fa893ace5316e34e1cd6b80518353","_shasum":"b1a4566f5396aaa97d4e29733b552baba651dd1c","bugs":{"url":"https://github.com/Blizzard/node-rdkafka/issues"},"contributors":[{"name":"Stephen Parente","email":"webmakersteve@gmail.com"},{"name":"Matt Gollob","email":"mattness@users.noreply.github.com"}],"directories":{},"dist-tags":{"latest":"1.0.6"},"dist":{"shasum":"b1a4566f5396aaa97d4e29733b552baba651dd1c","tarball":"https://registry.npmjs.org/node-rdkafka/-/node-rdkafka-1.0.6.tgz"},"maintainers":[{"name":"webmakersteve","email":"stephen@91ferns.com"}],"readmeFilename":"README.md","time":{"modified":"2017-08-05T15:39:02.483Z","created":"2016-08-11T18:05:07.341Z","0.3.0":"2016-08-11T18:05:07.341Z","0.3.1":"2016-08-17T19:03:51.613Z","0.3.2":"2016-08-25T23:24:02.791Z","0.3.3":"2016-08-26T01:15:24.640Z","0.4.0":"2016-09-22T14:37:31.700Z","0.4.1":"2016-09-22T23:59:20.191Z","0.4.2":"2016-09-23T00:58:34.881Z","0.5.0-ALPHA":"2016-09-28T17:25:13.035Z","0.5.0-BETA":"2016-09-28T17:26:39.166Z","0.5.0":"2016-09-28T17:50:46.295Z","0.5.1":"2016-09-29T00:42:32.782Z","0.5.2":"2016-10-04T22:05:10.218Z","0.5.3":"2016-10-04T23:13:41.051Z","0.5.4":"2016-10-05T21:21:15.743Z","0.5.5":"2016-10-11T00:40:57.740Z","0.6.0":"2016-11-15T01:14:49.039Z","0.6.1":"2016-11-22T19:04:28.015Z","0.6.2":"2016-12-16T01:46:42.565Z","0.6.3":"2016-12-23T00:17:54.299Z","0.6.4":"2017-01-18T21:05:09.402Z","0.7.0-ALPHA.2":"2017-01-19T21:38:00.113Z","0.7.0-ALPHA.3":"2017-02-08T21:06:57.466Z","0.8.0":"2017-02-27T23:24:02.659Z","0.8.1":"2017-03-21T21:08:28.200Z","0.8.2":"2017-03-21T21:40:35.905Z","0.8.2-TWFIX":"2017-03-21T22:41:03.841Z","0.9.0":"2017-03-22T21:38:05.071Z","0.10.0":"2017-03-25T06:29:40.713Z","0.10.1":"2017-04-27T07:32:33.880Z","0.10.2":"2017-04-27T15:46:32.996Z","1.0.0":"2017-06-01T20:12:30.406Z","1.0.1":"2017-06-20T16:37:39.834Z","1.0.2":"2017-06-22T16:38:08.997Z","1.0.3":"2017-06-23T03:44:42.237Z","1.0.4":"2017-06-23T19:36:47.914Z","1.0.6":"2017-07-18T03:59:12.240Z"},"users":{"gmarciani":true,"mano.rajesh":true,"wkronmiller":true}}}